{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([[ 101, 5529, 3236, 2523, 1599, 3614, 1391, 7649,  102],\n",
      "        [ 101, 3330, 3209,  679, 1599, 3614, 2802, 4413,  102]])\n",
      "<class 'torch.Tensor'> tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "<class 'torch.Tensor'> tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from model import NerModel\n",
    "from transformers import BertTokenizerFast,BertModel\n",
    "import torch\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"./premodels/bert-base-chinese/\")\n",
    "    t = tokenizer(\n",
    "        [\"胡晓很喜欢吃饭\",\"李明不喜欢打球\"],\n",
    "        return_tensors=\"pt\")\n",
    "    labels = [\n",
    "        [-100,0,0,1,1,1,0,0,-100],\n",
    "        [-100,0,0,1,1,1,0,0,-100]\n",
    "    ]\n",
    "    labels = torch.Tensor(labels)\n",
    "    input_ids = t[\"input_ids\"]\n",
    "    print(type(input_ids),input_ids)\n",
    "    token_type_ids = t[\"token_type_ids\"]\n",
    "    print(type(token_type_ids),token_type_ids)\n",
    "    attention_mask = t[\"attention_mask\"]\n",
    "    print(type(attention_mask),attention_mask)\n",
    "    total_loss =0\n",
    "    model = NerModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 46])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(input_ids,attention_mask)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "labels = labels.view(-1)\n",
    "labels = labels.long()\n",
    "print(labels.shape)\n",
    "loss =criterion(outputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.751842498779297\n"
     ]
    }
   ],
   "source": [
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testwords = \"张三不喜欢拉屎\"\n",
    "test_tokens = tokenizer(testwords,return_tensors=\"pt\")\n",
    "o = model(test_tokens[\"input_ids\"],test_tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46])\n"
     ]
    }
   ],
   "source": [
    "print(o[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
